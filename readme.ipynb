{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working on predicting the win_probability of a congressional candidate depending upon the forecast data. We will consider all the features that we might think be influential for the target value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/house_district_forecast.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get to know the data\n",
    "\n",
    "We are going to see how our data looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets take a look at the \"party\" column distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['party'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get to know your features\n",
    "\n",
    "We are going to look at how the columns are skewed using histogram. After looking at the individual column data we might want to do feature scaling or feature aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot = df.drop(columns=['special','incumbent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot.hist(bins=50,figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we got to know is few of them are skewed to left or have outliers. Since the district feature doesnt make sense on its own, we might want to aggregate it with state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"state_district\"] = df[\"state\"].map(str)+\"_\"+df[\"district\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"voteshare\"].value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To categorize the data for voteshare we create a new column called votershare. In order to keep the no of categories to low we divide the voteshare by 10. we mainly calculate this to get stratified sampling everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df[\"voteshare_cat\"] = np.ceil(df[\"voteshare\"]/10)\n",
    "df[\"voteshare_cat\"].value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spliting the data into tarinaing and test sets\n",
    "\n",
    "We generally split it with 20 to 80 ratio for test and trainig sets. To keep it startified we use the help of sikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "for train_index, test_index in split.split(df,df[\"voteshare_cat\"]):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in (strat_train_set,strat_test_set):\n",
    "    set.drop([\"voteshare_cat\"], axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = data_train.corr()\n",
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between features\n",
    "\n",
    "We use correlation matrix to keep or eliminate feature that the data shows might influence our target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix[\"win_probability\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"win_probability\",\"state_district\",\"party\",\"voteshare\",\"p10_voteshare\",\"p90_voteshare\"]\n",
    "scatter_matrix(data_train[attributes],figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see a strong correlation between winning probability and voteshare a candidate has in their respective districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"voteshare\", y=\"win_probability\",alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data has no null values except for \"special\" column, we are dropping the column. As it is has no correlation with win_probability too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(\"special\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two dataframes out of the df which has predictors and labels. As we dont want to apply transformation for target values. And we are dropping all the columns that might be irrelevant as per the correlation matrix too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictors = strat_train_set.drop([\"win_probability\",\"forecastdate\",\"special\",\"candidate\",\"incumbent\",\"model\",\"p10_voteshare\",\"p90_voteshare\"],axis=1)\n",
    "df_predictors_labels = strat_train_set[\"win_probability\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make use of text data we need to convert it into numerical form. Here we will convert state, party into numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder_party = LabelEncoder()\n",
    "party_cat = df_predictors[\"party\"]\n",
    "party_cat_encoded = encoder_party.fit_transform(party_cat.astype(str))\n",
    "party_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_party.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "state_cat = df_predictors[\"state\"]\n",
    "state_cat_encoded = encoder.fit_transform(state_cat.astype(str))\n",
    "state_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating two categories of columns in-order to transform them in data pipeline. We are removing all the non-numerical columns and also the target value column \"win_probability\". We have only categorial columns that are strings \"party\" and \"state_district\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes = [\"party\",\"state_district\",\"state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline\n",
    "\n",
    "Generally in the pipeline we an use different transformations that ssklearn provides us with but since data seems tobe pretty consistent for this dataset. So we wont be using imputer or standard scaler. But we will be using LabelEncoder for string categorial data. FaetureUnion from scikit learn makes it simple to run multiple tranformation on data parallely.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion,make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have multiple label columns, we might need a multiple column label encoder. sklearn supports multilabelencoder but not multilabelencoder. The solution is influenced by this answer in stackoverflow (https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn). Check \"data.py\" for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import MultiColumnLabelEncoder\n",
    "df_predictors_prepared = MultiColumnLabelEncoder(cat_attributes).fit_transform(df_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try out some models\n",
    "\n",
    "Since we are predicting a target numeric value this falls under regression task. We can use Linear regression, multivariate regression, Decision Trees and Random Forests. Let's try out some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linReg = LinearRegression()\n",
    "linReg.fit(df_predictors_prepared,df_predictors_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_data = df_predictors.iloc[:8]\n",
    "ex_label = df_predictors_labels.iloc[:8]\n",
    "ex_prepared = MultiColumnLabelEncoder(cat_attributes).fit_transform(ex_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linReg.predict(ex_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ex_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "df_predictions = linReg.predict(df_predictors_prepared)\n",
    "linMse = mean_squared_error(df_predictors_labels,df_predictions)\n",
    "linRmse = np.sqrt(linMse)\n",
    "linRmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experimented with Linear regression here. We took a sample data of 8 samples and predicted their target values. And compared them to the actual values. We can see there is lot of descrepency. So lets try Decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtreeReg = DecisionTreeRegressor()\n",
    "dtreeReg.fit(df_predictors_prepared,df_predictors_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = dtreeReg.predict(df_predictors_prepared)\n",
    "dtreeMse = mean_squared_error(df_predictors_labels,df_predictions)\n",
    "dtreeRmse = np.sqrt(dtreeMse)\n",
    "dtreeRmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is surprisingly low. But we may have overfit the model. Lets try it with Random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfReg = RandomForestRegressor()\n",
    "rfReg.fit(df_predictors_prepared,df_predictors_labels)\n",
    "df_predictions = rfReg.predict(df_predictors_prepared)\n",
    "rfRegMse = mean_squared_error(df_predictors_labels,df_predictions)\n",
    "rfRegRmse = np.sqrt(rfRegMse)\n",
    "rfRegRmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final test\n",
    "Now is the time to test it with test data. Lets finalize the model to be Random Forests. Keep in mind this might not be a real life scenario as you may want to tweak a bit till you find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = strat_test_set.drop([\"win_probability\",\"forecastdate\",\"special\",\"candidate\",\"incumbent\",\"model\"],axis=1)\n",
    "y_test = strat_test_set[\"win_probability\"].copy()\n",
    "X_test_prepared = MultiColumnLabelEncoder(cat_attributes).fit_transform(X_test)\n",
    "final_predictions = rfReg.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test,final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
